{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc296f7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributed RDataFrame\n",
    "\n",
    "An `RDataFrame` analysis written in Python can be executed both *locally* - possibly in parallel on the cores of the machine - and *distributedly* by offloading computations to external resources, which include:\n",
    "\n",
    "- [Spark](https://spark.apache.org/) and \n",
    "- [Dask](https://dask.org/) clusters. \n",
    "\n",
    "- This feature is enabled by the architecture depicted below.\n",
    "\n",
    "- It shows that RDataFrame computation graphs can be mapped to different kinds of resources via backends.\n",
    "\n",
    "- In this notebook we will exercise the Dask backend, which divides an `RDataFrame` input dataset in logical ranges and submits computations for each of those ranges to Dask resources.\n",
    "\n",
    "<img src=\"../../images/DistRDF_architecture.png\" alt=\"Distributed RDataFrame\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2731e8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a Dask client (in a dummy `LocalCluster` created inside the notebook)\n",
    "\n",
    "- In order to work with a Dask cluster we need a `Client` object.\n",
    "- It represents the connection to that cluster and allows to configure execution-related parameters (e.g. number of cores, memory). \n",
    "- The client object is just the intermediary between our client session and the cluster resources.\n",
    "- Dask supports many different resource managers.\n",
    "- We will follow the [Dask documentation](https://distributed.dask.org/en/stable/client.html) regarding the creation of a `Client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e156e5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from distributed import Client, LocalCluster\n",
    "# A LocalCluster creates a test Cluster, segmenting the resources available under your notebook\n",
    "# It is meant for prototyping purposes and will not give full performance\n",
    "cluster = LocalCluster(n_workers=2, threads_per_worker=1, processes=True, memory_limit=\"2GiB\")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "601ffed1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a ROOT dataframe\n",
    "\n",
    "We now create a distributed RDataFrame with Dask. It accepts two more keyword arguments:\n",
    "- the number of partitions to apply to the dataset (`npartitions`).\n",
    "- the `Client` object (`daskclient`).\n",
    "\n",
    "Besides these details, a Dask RDataFrame is not different from a local RDataFrame: the analysis presented in this notebook would not change if we wanted to execute it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cf12f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use a Dask RDataFrame\n",
    "RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "\n",
    "df = RDataFrame(\"h42\",\n",
    "                \"https://root.cern/files/h1big.root\",\n",
    "                npartitions=4,\n",
    "                daskclient=client)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "563a28e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run your analysis unchanged\n",
    "\n",
    "- From now on, the rest of your application can be written **exactly** as we have seen with local RDataFrame. \n",
    "\n",
    "- The goal of the distributed RDataFrame module is to support all the traditional RDataFrame operations (those that make sense in a distributed context at least). \n",
    "\n",
    "- Currently only a subset of those is available and can be found in the corresponding [section of the documentation](https://root.cern/doc/master/classROOT_1_1RDataFrame.html#distrdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43562f8d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df1 = df.Filter(\"nevent > 1\")\n",
    "df2 = df1.Define(\"mpt\",\"sqrt(xpt*xpt + ypt*ypt)\")\n",
    "c = df.Count()\n",
    "m = df2.Mean(\"mpt\")\n",
    "x = df2.Mean(\"xpt\")\n",
    "y = df2.Mean(\"ypt\")\n",
    "E = df2.Mean(\"Ept\")\n",
    "dept = df2.Mean(\"dept\")\n",
    "dxpt = df2.Mean(\"dxpt\")\n",
    "dypt = df2.Mean(\"dypt\")\n",
    "de33 = df2.Mean(\"de33\")\n",
    "print(f\"Number of events after processing: {c.GetValue()}\")\n",
    "print(f\"Mean of column 'mpt': {m.GetValue()}\")\n",
    "print(f\"Mean of column 'xpt': {x.GetValue()}\")\n",
    "print(f\"Mean of column 'ypt': {y.GetValue()}\")\n",
    "print(f\"Mean of column 'Ept': {E.GetValue()}\")\n",
    "print(f\"Mean of column 'dept': {dept.GetValue()}\")\n",
    "print(f\"Mean of column 'dxpt': {dxpt.GetValue()}\")\n",
    "print(f\"Mean of column 'dypt': {dypt.GetValue()}\")\n",
    "print(f\"Mean of column 'de33': {de33.GetValue()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "714444c1-0b61-4be0-8600-b7aae23ba408",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create a Dask client (in the `KubeCluster` available in the Analysis Facility!) \n",
    "\n",
    "Now, we are ready to unleash the power of the Analysis Facility. \n",
    "Instead of using the dummy `LocalCluster`, which simply uses the resources underneath the jupyter lab instance, we are going to use the Cluster of the high-rate platform (with greater firepower).\n",
    "\n",
    "To create a `KubeCluster`:\n",
    "- Click on the Dask icon, in the left bar of the JupyterLab instance: should look like this <img src=\"../../images/dask.png\" alt=\"Dask icon\" width=\"20\">\n",
    "- Click on the <img src=\"../../images/new.png\" alt=\"new\" width=\"45\"> button, in the bottom part of the column;\n",
    "- Wait for the cluster to deploy (a few seconds);\n",
    "- When a dashboard (with several buttons) appears, scroll down until you see a blue box like:\n",
    "  <img src=\"../../images/kubecluster.png\" alt=\"Kube cluster\" width=\"500\">\n",
    "- Now, to be able to create a `Client` object, as shown in the previous part, just click on the <img src=\"../../images/client.png\" alt=\"client\" width=\"20\"> icon. A new notebook cell will appear in the Jupyter notebook with all the details of the new Cluster!\n",
    "\n",
    "Before you run the computation on the `KubeCluster`, <ins>you need to scale it with some *Workers*</ins>. To do so, click on the <span style=\"color:green\">**SCALE**</span> button, in the blue box. In the window that appears, put the number of workers you need (every worker has a single core with 2GB of RAM).\n",
    "\n",
    "**NOTE**: For this workshop, we are sharing the same resources! Therefore, don't exaggerate with the number of workers! For the sake of this \"light\" examples, <ins>2-3 workers are enough</ins>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a81a0-24c9-4e65-bf71-3afd9c5a8265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOMETHING LIKE THIS SHOULD APPEAR ON YOUR NOTEBOOK. This has been created on a previous cluster,\n",
    "# so replace it with your Client information.\n",
    "\n",
    "#from dask.distributed import Client\n",
    "\n",
    "#client = Client(\"tcp://dask-diotalevi-gfibr-scheduler.jhub:8786\")\n",
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cd0133-57c5-4f22-b0b3-ea3344c4cb10",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use a Dask RDataFrame\n",
    "RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "\n",
    "df = RDataFrame(\"h42\",\n",
    "                \"https://root.cern/files/h1big.root\",\n",
    "                npartitions=4,\n",
    "                daskclient=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f891f599-7bde-41fd-8483-ea4a639fa007",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df1 = df.Filter(\"nevent > 1\")\n",
    "df2 = df1.Define(\"mpt\",\"sqrt(xpt*xpt + ypt*ypt)\")\n",
    "c = df.Count()\n",
    "m = df2.Mean(\"mpt\")\n",
    "x = df2.Mean(\"xpt\")\n",
    "y = df2.Mean(\"ypt\")\n",
    "E = df2.Mean(\"Ept\")\n",
    "dept = df2.Mean(\"dept\")\n",
    "dxpt = df2.Mean(\"dxpt\")\n",
    "dypt = df2.Mean(\"dypt\")\n",
    "de33 = df2.Mean(\"de33\")\n",
    "print(f\"Number of events after processing: {c.GetValue()}\")\n",
    "print(f\"Mean of column 'mpt': {m.GetValue()}\")\n",
    "print(f\"Mean of column 'xpt': {x.GetValue()}\")\n",
    "print(f\"Mean of column 'ypt': {y.GetValue()}\")\n",
    "print(f\"Mean of column 'Ept': {E.GetValue()}\")\n",
    "print(f\"Mean of column 'dept': {dept.GetValue()}\")\n",
    "print(f\"Mean of column 'dxpt': {dxpt.GetValue()}\")\n",
    "print(f\"Mean of column 'dypt': {dypt.GetValue()}\")\n",
    "print(f\"Mean of column 'de33': {de33.GetValue()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4970fd3",
   "metadata": {},
   "source": [
    "## Upload a custom header and declare the code on the workers\n",
    "\n",
    "In the situation where custom C++ code is written in a separate header file to be used in the analysis, we need to make sure that all the workers have the header file (or any other ancillary file) in their filesystem. This is currently achievable by:\n",
    "\n",
    "1. Register the files for upload via the `distribute_unique_paths` method\n",
    "2. Register the function that uses that file with the `initialize` function\n",
    "\n",
    "```python\n",
    "import ROOT\n",
    "from pathlib import Path\n",
    "from distributed import get_worker\n",
    "\n",
    "# Just an example, imagine this is a real distributed RDataFrame\n",
    "df_dask = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame(...)\n",
    "\n",
    "# We need to register the header file for upload to the Dask workers.\n",
    "# For now the interface is still WIP, will be made smoother in the next ROOT release\n",
    "df_dask._headnode.backend.distribute_unique_paths(\n",
    "    [\"mycustomheader.h\"]\n",
    ")\n",
    "\n",
    "def my_initialization_function():\n",
    "    \"\"\"Load C++ helper functions. Works for both local and distributed execution.\"\"\"\n",
    "    try:\n",
    "        # when using distributed RDataFrame 'mycustomheader.h' is copied to the local_directory\n",
    "        # of every worker (via `distribute_unique_paths`)\n",
    "        localdir = get_worker().local_directory\n",
    "        cpp_header = Path(localdir) / \"mycustomheader.h\"\n",
    "    except ValueError:\n",
    "        # must be local execution\n",
    "        cpp_header = \"mycustomheader.h\"\n",
    "\n",
    "    ROOT.gInterpreter.Declare(f'#include \"{str(cpp_header)}\"')\n",
    "\n",
    "ROOT.RDF.Experimental.Distributed.initialize(my_initialization_function)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9ac34",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
